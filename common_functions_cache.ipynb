{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Version of common functions with more caching, also using new sqlite db schema (LTCC_cache). Functions for querying abstracts on ePMC, saving results in sqlite db and displaying results in dataframe.\\nMore checks in this version to avoid duplicate retrieval of information from web services. \\nFunctions available:\\ncreate_db(db_name)\\npop_chembl_pmids(db_name) -- this populates the chembl_pmids table with pmids from a specific chembl_version.\\ndef define_synonym_queries(term_dict_1, term_dict_2 = None, term_dict_3 = None)\\nget_hit_profile(query_list)\\nget_pmids(query, db_name) -- query is string\\nget_article_data(query_id, db_name) -- query is string\\nget_pmids_and_article_data(query, db_name) -- query is string\\nget_availabilities(query_id, db_name)\\nget_df(query_id_list, db_name, sql_condition=None)\\nseparate_column_df(query_id)\\ncolour_terms(df, markup_list)\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Version of common functions with more caching, also using new sqlite db schema (LTCC_cache). Functions for querying abstracts on ePMC, saving results in sqlite db and displaying results in dataframe.\n",
    "More checks in this version to avoid duplicate retrieval of information from web services. \n",
    "Functions available:\n",
    "create_db(db_name)\n",
    "pop_chembl_pmids(db_name) -- this populates the chembl_pmids table with pmids from a specific chembl_version.\n",
    "def define_synonym_queries(term_dict_1, term_dict_2 = None, term_dict_3 = None)\n",
    "get_hit_profile(query_list)\n",
    "get_pmids(query, db_name) -- query is string\n",
    "get_article_data(query_id, db_name) -- query is string\n",
    "get_pmids_and_article_data(query, db_name) -- query is string\n",
    "get_availabilities(query_id, db_name)\n",
    "get_scores(query_id, db_name)\n",
    "set_chembl_values(query_id, db_name)\n",
    "get_df(query_id_list, db_name, sql_condition=None)\n",
    "separate_column_df(query_id)\n",
    "colour_terms(df, markup_list)\n",
    "plot_scores(query_id_list, db_name)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sqlite3 as lite\n",
    "import datetime\n",
    "import requests\n",
    "from lxml import etree\n",
    "import cx_Oracle\n",
    "import re\n",
    "import shelve\n",
    "import base64\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import sqlite3 as lite\n",
    "import datetime\n",
    "from IPython.display import HTML\n",
    "import lxml.html\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_db(db_name):\n",
    "    '''Create the SQLite database in the current directory.\n",
    "    kwargs: db_name -- name of the new SQLite db you are creating'''\n",
    "    \n",
    "    conn = lite.connect(db_name)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    #create various tables\n",
    "    cursor.execute(\"create table queries(query_id integer primary key, query text, hitcount integer, date_performed text)\")\n",
    "    cursor.execute(\"create table result_ids(query_id integer, pmid integer, primary key(query_id, pmid))\")\n",
    "    cursor.execute(\"create table article_data(pmid integer primary key, year integer, title text, abstract text, journal_title text, journal_abbrev_title text, in_epmc integer, avail_codes text, pdf_links text, other_links text, in_chembl int)\")\n",
    "    cursor.execute(\"create table article_links(pmid integer primary key, campus_links text, request_access text)\")\n",
    "    \n",
    "    cursor.execute(\"create table scores(pmid integer primary key, score real)\")\n",
    "    cursor.execute(\"create table chembl_pmids(pmid integer primary key)\")\n",
    "    \n",
    "    cursor.execute(\"create table error_records(query_id integer, object_id text, pmid integer, error_comment text)\")\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pop_chembl_pmids(db_name):\n",
    "    '''Get all pubmed ids from ChEMBL_20 and populate the chembl_pmids table in the SQLite db with those pmids. Needs a file with login details to ChEMBL database.\n",
    "    kwargs: db_name -- name of the SQLite db of which the chembl_pmids table should be updated'''\n",
    "    \n",
    "    # get login details from file\n",
    "    fileObj = open('/homes/ines/chembl_20_login_details.txt')\n",
    "    database = fileObj.read().strip()\n",
    "    fileObj.close()\n",
    "    \n",
    "    conn = cx_Oracle.connect(database)\n",
    "    cursor = conn.cursor()\n",
    "    chembl_pmid_list = []\n",
    "    \n",
    "    # get all pmids in chembl_20. Need to redo this for new version of chembl\n",
    "    sql = '''\n",
    "    select distinct pubmed_id from docs where pubmed_id is not null\n",
    "    '''\n",
    "    cursor.execute(sql)\n",
    "    \n",
    "    chembl_pmids_list = [pmid_tuple[0] for pmid_tuple in cursor.fetchall()]\n",
    "\n",
    "    conn.close()\n",
    "    \n",
    "    # open queries_db\n",
    "    conn = lite.connect(db_name)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # insert pmids in chembl_pmids table\n",
    "    for pmid in chembl_pmids_list:\n",
    "        cursor.execute(\"insert or ignore into chembl_pmids(pmid) values(?)\", (pmid,))\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def define_synonym_queries(term_dict_1, term_dict_2 = None, term_dict_3 = None):\n",
    "    \"\"\"Submit one, two or three dicts with synonyms/alternative terms to be used. \n",
    "    If there are no synonyms bu just one term, just include the primary term as one item in the list that is that value of the dict.\n",
    "    For each dict the terms in the dict will be joined by OR. The three different dicts will be joined by AND in the query.\n",
    "    Returns a list of queries. It is possible to have only one item in a dictionary, if you only want to do one query.\n",
    "    Otherwise include the independent items to look for as different items in the dictionary.\n",
    "    \n",
    "    kwargs: term_dict_1 -- dict of terms (in list in value of dict) to be connected with OR in the query\n",
    "            term_dict_2 -- additional terms to be included in the same query (default = None)\n",
    "            term_dict_3 -- even more terms to be included in the same query (default = None)\n",
    "    \"\"\"\n",
    "\n",
    "    my_queries = []\n",
    "    \n",
    "    if term_dict_2 == None:\n",
    "    \n",
    "        for term in term_dict_1:\n",
    "            query_terms = '\"'+'\" OR ABSTRACT:\"'.join(term_dict_1[term])+'\"'\n",
    "            query = '(ABSTRACT:'+query_terms+')'\n",
    "            my_queries.append(query)\n",
    "\n",
    "    elif term_dict_2 != None and term_dict_3 == None:\n",
    "        \n",
    "        for term in term_dict_1:\n",
    "            for term_2 in term_dict_2:\n",
    "                    \n",
    "                query_dict_1 = '\"'+'\" OR ABSTRACT:\"'.join(term_dict_1[term])+'\"'\n",
    "                query_dict_2 = '\"'+'\" OR ABSTRACT:\"'.join(term_dict_2[term_2])+'\"'\n",
    "                query = '(ABSTRACT:'+query_dict_1+')'+' AND '+'(ABSTRACT:'+query_dict_2+')'\n",
    "                my_queries.append(query)\n",
    "        \n",
    "    elif term_dict_3 != None:\n",
    "        \n",
    "        for term in term_dict_1:\n",
    "            for term_2 in term_dict_2:\n",
    "                for term_3 in term_dict_3:\n",
    "                    \n",
    "                    query_dict_1 = '\"'+'\" OR ABSTRACT:\"'.join(term_dict_1[term])+'\"'\n",
    "                    query_dict_2 = '\"'+'\" OR ABSTRACT:\"'.join(term_dict_2[term_2])+'\"'\n",
    "                    query_dict_3 = '\"'+'\" OR ABSTRACT:\"'.join(term_dict_3[term_3])+'\"'\n",
    "                    query = '(ABSTRACT:'+query_dict_1+')'+' AND '+'(ABSTRACT:'+query_dict_2+')'+' AND '+'(ABSTRACT:'+query_dict_3+')'\n",
    "                    my_queries.append(query)\n",
    "        \n",
    "    return my_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_hit_profile(query_list):\n",
    "    \n",
    "    \"\"\"For each query in query_list print profile hits. This uses the profile module of ePMC webservices. Will print results to console.\n",
    "    kwargs: query_list -- list of queries (strings), e.g from define_synonym_queries function.\"\"\"\n",
    "    \n",
    "    st_dict = {}\n",
    "    base = 'http://www.ebi.ac.uk/europepmc/webservices/rest/profile/query=({} AND (src:MED OR src:PMC OR src:CTX))'\n",
    "    total_nr_articles = 0\n",
    "    \n",
    "    for query in query_list:\n",
    "            \n",
    "        response = requests.get(base.format(query))\n",
    "        tree = etree.fromstring(response.content)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print('for query:{} status_code: {}'.format(query, str(response.status_code)))\n",
    "            \n",
    "        all_result = tree.xpath('/responseWrapper/profileList/pubType[@name=\"ALL\"]')\n",
    "        ft_result = tree.xpath('/responseWrapper/profileList/pubType[@name=\"FULL TEXT\"]')\n",
    "            \n",
    "        all_string = str(etree.tostring(all_result[0], encoding = 'unicode'))\n",
    "        ft_string = str(etree.tostring(ft_result[0], encoding = 'unicode'))\n",
    "                \n",
    "        if re.search(r'count=\"0\"', all_string) == None:\n",
    "            st_dict[query] = {'all':int(float(re.search(r'count=\"([0-9]+)\"', all_string).group(1))), 'full text':re.search(r'count=\"([0-9]+)\"', ft_string).group(1)}\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    for entry in st_dict:\n",
    "        print(entry)\n",
    "        print(st_dict[entry])\n",
    "        total_nr_articles = total_nr_articles + st_dict[entry]['all']\n",
    "    \n",
    "    print('\\n'+'total number of hits = '+str(total_nr_articles))\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_pmids(query, db_name):\n",
    "    '''For given query (string) get results in forms of idlist from ePMC. Record the query and date in the queries table. Save pmids associated with the query in the result_ids table.\n",
    "    It will be better to use the get_pmids_and_article data function, but if only getting the pmids is required for quick overlap checking, for example, then use this function.\n",
    "    Return the query_id assigned to the query in the queries table, can then be used in subsequent functions.\n",
    "    kwargs: query -- should be string\n",
    "            db_name\n",
    "    '''\n",
    "    \n",
    "    moment = datetime.datetime.now()\n",
    "    current_date = \"{}-{}-{}\".format(moment.year, moment.month, moment.day)\n",
    "    \n",
    "    base = 'http://www.ebi.ac.uk/europepmc/webservices/rest/search/query=({} AND (src:MED OR src:PMC OR src:CTX))&resulttype=idlist&page={}'\n",
    "    \n",
    "    conn = lite.connect(db_name)    \n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    def do_query(my_query, page_nr = 1):\n",
    "        \n",
    "        response = requests.get(base.format(my_query, page_nr))\n",
    "        \n",
    "        assert response.status_code == 200\n",
    "        \n",
    "        tree = etree.fromstring(response.content)\n",
    "        \n",
    "        return tree\n",
    "    \n",
    "    def save_pmid(result):\n",
    "        \n",
    "        pmid = result.xpath('pmid/text()')[0]\n",
    "        \n",
    "        cursor.execute(\"insert or ignore into result_ids(query_id, pmid) values (?,?)\", (current_query_id, pmid))\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    \n",
    "    tree = do_query(query)\n",
    " \n",
    "    result_hitcount = int(tree.xpath('/responseWrapper/hitCount/text()')[0]) \n",
    "    cursor.execute(\"insert into queries(query_id, query, hitcount, date_performed) values (NULL,?,?,?)\", (query, result_hitcount, current_date))\n",
    "    current_query_id = cursor.lastrowid\n",
    "\n",
    "\n",
    "    result_pmids = tree.xpath('/responseWrapper/resultList/result')\n",
    "\n",
    "\n",
    "    for item in result_pmids:\n",
    "\n",
    "        try:\n",
    "            save_pmid(item)\n",
    "        except IndexError:\n",
    "            object_id = item.xpath('id/text()')[0]\n",
    "            error_comment = '(get_pmids) - IndexError with XML, possibly no pmid for this item or no journal info e.g. when is book chapter'\n",
    "            cursor.execute(\"insert or ignore into error_records(query_id, object_id, error_comment) values (?,?,?)\", (current_query_id, object_id, error_comment))\n",
    "            continue\n",
    "\n",
    "\n",
    "    if (result_hitcount % 25 == 0) and (result_hitcount / 25 > 1): # hitCount is multiple of 25 (remainder==0) but more than one page\n",
    "        total_pages = int(result_hitcount / 25)\n",
    "\n",
    "    else:  \n",
    "        total_pages = (int(result_hitcount // 25)) + 1\n",
    "\n",
    "    for page in range(2,total_pages+1): #page one was already done so continue from page 2 and add results to same dictionary\n",
    "\n",
    "        tree = do_query(query, page_nr = page)\n",
    "        result_pmids = tree.xpath('/responseWrapper/resultList/result')\n",
    "\n",
    "        for item in result_pmids:\n",
    "\n",
    "            try:\n",
    "                save_pmid(item)\n",
    "            except IndexError:\n",
    "                object_id = item.xpath('id/text()')[0]\n",
    "                error_comment = '(get_pmids) - IndexError with XML, possibly no pmid for this item or no journal info e.g. when is book chapter'\n",
    "                cursor.execute(\"insert or ignore into error_records(query_id, object_id) values (?,?,?)\", (current_query_id, object_id, error_comment))\n",
    "                continue\n",
    "                        \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "    return current_query_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_article_data(query_id, db_name):\n",
    "    '''If already perfomed the get_pmids function for a query, now go in more detail and get the article data associated with the pmids form ePMC.\n",
    "    Checks whether the pmid already exists in the article_data table and if so, skips that one and does not extract article data from ePMC.\n",
    "    Returns None. \n",
    "    kwargs: query_id -- (obtained from get_pmids function)\n",
    "            db_name'''\n",
    "    \n",
    "    conn = lite.connect(db_name)    \n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    base = 'http://www.ebi.ac.uk/europepmc/webservices/rest/search/query=({} AND (src:MED OR src:PMC OR src:CTX))&resulttype=core&page={}'\n",
    "    \n",
    "    cursor.execute('select query from queries where query_id = ?', (query_id,))\n",
    "    query = cursor.fetchall()[0][0]\n",
    "    \n",
    "    \n",
    "\n",
    "    def do_query(my_query, page_nr = 1):\n",
    "\n",
    "        response = requests.get(base.format(my_query, page_nr))\n",
    "        assert response.status_code == 200, \"status code != 200\"\n",
    "    \n",
    "        tree = etree.fromstring(response.content)\n",
    "        return tree\n",
    "\n",
    "    def retrieve_data(result):\n",
    "    \n",
    "        #pmid = result.xpath('pmid/text()')[0]\n",
    "        title = result.xpath('title/text()')[0]\n",
    "        abstract = result.xpath('abstractText/text()')[0]\n",
    "        year = result.xpath('journalInfo/yearOfPublication/text()')[0]\n",
    "        journal_title = result.xpath('journalInfo/journal/title/text()')[0]\n",
    "        journal_abbrev_title = result.xpath('journalInfo/journal/medlineAbbreviation/text()')[0]\n",
    "        \n",
    "        if result.xpath('inEPMC/text()')[0] == 'Y': \n",
    "            in_ePMC = 1 \n",
    "        else: \n",
    "            in_ePMC = 0\n",
    "\n",
    "        url_list = []\n",
    "        avail_code_list = []\n",
    "        doc_style_list = []\n",
    "        other_links_list = []\n",
    "        pdf_links_list = []\n",
    "\n",
    "        ft_url_list = result.xpath('fullTextUrlList/fullTextUrl')\n",
    "        \n",
    "        for url_item in ft_url_list:\n",
    "            avail_code = url_item.xpath('availabilityCode/text()')[0]\n",
    "            avail_code_list.append(avail_code)\n",
    "            doc_style = url_item.xpath('documentStyle/text()')[0]\n",
    "            doc_style_list.append(doc_style)\n",
    "\n",
    "            if avail_code != 'S' and doc_style != 'pdf':\n",
    "                url = url_item.xpath('url/text()')[0]\n",
    "                other_links_list.append(url)\n",
    "\n",
    "        if 'pdf' in doc_style_list:\n",
    "            for url_item in ft_url_list:\n",
    "                doc_style = url_item.xpath('documentStyle/text()')[0]\n",
    "                if doc_style == 'pdf':\n",
    "                    url = url_item.xpath('url/text()')[0]\n",
    "                    pdf_links_list.append(url)                        \n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        avail_codes = ', '.join(set(avail_code_list)) if avail_code_list else None\n",
    "        other_links = ', '.join(other_links_list) if other_links_list else None\n",
    "        pdf_links = ', '.join(pdf_links_list) if pdf_links_list else None\n",
    "        \n",
    "        cursor.execute(\"insert or ignore into article_data(pmid, year, title, abstract, journal_title, journal_abbrev_title, in_epmc, avail_codes, pdf_links, other_links) values (?,?,?,?,?,?,?,?,?,?)\", (pmid, year, title, abstract, journal_title, journal_abbrev_title, in_ePMC, avail_codes, pdf_links, other_links))\n",
    "        \n",
    "        conn.commit()\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    # now start the tasks\n",
    "    \n",
    "    cursor.execute('select r.pmid from result_ids r where query_id = ? and r.pmid in (select distinct a.pmid from article_data a)', (query_id,))\n",
    "    existing_pmid_list = [i[0] for i in cursor.fetchall()]\n",
    "\n",
    "    tree = do_query(query)\n",
    "    \n",
    "    result_abstracts = tree.xpath('/responseWrapper/resultList/result')\n",
    "    result_hitcount = int(tree.xpath('/responseWrapper/hitCount/text()')[0]) \n",
    "\n",
    "    for item in result_abstracts:\n",
    "        try:\n",
    "            pmid = int(item.xpath('pmid/text()')[0])\n",
    "\n",
    "            if pmid in existing_pmid_list:\n",
    "                continue\n",
    "\n",
    "            else:\n",
    "                try:\n",
    "                    retrieve_data(item)\n",
    "                except IndexError:\n",
    "                    object_id = item.xpath('id/text()')[0]\n",
    "                    error_comment = '(get_article_data) - IndexError with XML, possibly field not present, e.g. no journal info e.g. when is book chapter'\n",
    "                    cursor.execute(\"insert or ignore into error_records(query_id, object_id, error_comment) values (?,?,?)\", (current_query_id, object_id, error_comment))\n",
    "                    continue\n",
    "        except IndexError:\n",
    "            object_id = item.xpath('id/text()')[0]\n",
    "            error_comment = '(get_article_data) - IndexError with XML, possibly no pmid for this item'\n",
    "            cursor.execute(\"insert or ignore into error_records(query_id, object_id, error_comment) values (?,?,?)\", (current_query_id, object_id, error_comment))\n",
    "            continue\n",
    "        \n",
    "    if (result_hitcount % 25 == 0) and (result_hitcount / 25 > 1): # hitCount is multiple of 25 (remainder==0) but more than one page\n",
    "        total_pages = int(result_hitcount / 25)\n",
    "\n",
    "    else:  \n",
    "        total_pages = (int(result_hitcount // 25)) + 1 # this holds for cases where there is only one page as well\n",
    "        \n",
    "    for page in range(2,total_pages+1): #page one was already done so continue from page 2 and add results to same dictionary\n",
    "\n",
    "        tree = do_query(query, page_nr = page)\n",
    "        result_abstracts = tree.xpath('/responseWrapper/resultList/result')\n",
    "\n",
    "        for item in result_abstracts:\n",
    "            try:\n",
    "                pmid = int(item.xpath('pmid/text()')[0])\n",
    "\n",
    "                if pmid in existing_pmid_list:\n",
    "                    continue\n",
    "\n",
    "                else:\n",
    "                    try:\n",
    "                        retrieve_data(item)\n",
    "                    except IndexError:\n",
    "                        object_id = item.xpath('id/text()')[0]\n",
    "                        error_comment = '(get_article_data) - IndexError with XML, possibly field not present, e.g. no journal info e.g. when is book chapter'\n",
    "                        cursor.execute(\"insert or ignore into error_records(query_id, object_id, error_comment) values (?,?,?)\", (current_query_id, object_id, error_comment))\n",
    "                        continue\n",
    "            except IndexError:\n",
    "                object_id = item.xpath('id/text()')[0]\n",
    "                error_comment = '(get_article_data) - IndexError with XML, possibly no pmid for this item'\n",
    "                cursor.execute(\"insert or ignore into error_records(query_id, object_id, error_comment) values (?,?,?)\", (current_query_id, object_id, error_comment))\n",
    "                continue\n",
    "\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_pmids_and_article_data(query, db_name):\n",
    "    '''Does same as get_pmids and get_article_data functions but in one step, which I think is faster than doing both separately. Uses the core resulttype from ePMC search module.\n",
    "    Does not exclude articles that are in chembl because that field is set later.\n",
    "    Return the query_id assigned to the query in the queries table, can then be used in subsequent functions.\n",
    "    kwargs:\n",
    "            query -- string\n",
    "            db_name -- name of SQLite db\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    moment = datetime.datetime.now()\n",
    "    current_date = \"{}-{}-{}\".format(moment.year, moment.month, moment.day)\n",
    "    \n",
    "    base = 'http://www.ebi.ac.uk/europepmc/webservices/rest/search/query=({} AND (src:MED OR src:PMC OR src:CTX))&resulttype=core&page={}'\n",
    "    \n",
    "    conn = lite.connect(db_name)    \n",
    "    cursor = conn.cursor()\n",
    "    new_inserted_count = 0\n",
    "    exist_count = 0\n",
    "    \n",
    "    def do_query(my_query, page_nr = 1):\n",
    "        \n",
    "        response = requests.get(base.format(my_query, page_nr))\n",
    "        \n",
    "        assert response.status_code == 200\n",
    "        \n",
    "        tree = etree.fromstring(response.content)\n",
    "        \n",
    "        return tree\n",
    "\n",
    "    def retrieve_data(result):\n",
    "    \n",
    "        #pmid = result.xpath('pmid/text()')[0]\n",
    "        title = result.xpath('title/text()')[0]\n",
    "        abstract = result.xpath('abstractText/text()')[0]\n",
    "        year = result.xpath('journalInfo/yearOfPublication/text()')[0]\n",
    "        journal_title = result.xpath('journalInfo/journal/title/text()')[0]\n",
    "        journal_abbrev_title = result.xpath('journalInfo/journal/medlineAbbreviation/text()')[0]\n",
    "        \n",
    "        if result.xpath('inEPMC/text()')[0] == 'Y': \n",
    "            in_ePMC = 1 \n",
    "        else: \n",
    "            in_ePMC = 0\n",
    "\n",
    "        url_list = []\n",
    "        avail_code_list = []\n",
    "        doc_style_list = []\n",
    "        other_links_list = []\n",
    "        pdf_links_list = []\n",
    "\n",
    "        ft_url_list = result.xpath('fullTextUrlList/fullTextUrl')\n",
    "        \n",
    "        for url_item in ft_url_list:\n",
    "            avail_code = url_item.xpath('availabilityCode/text()')[0]\n",
    "            avail_code_list.append(avail_code)\n",
    "            doc_style = url_item.xpath('documentStyle/text()')[0]\n",
    "            doc_style_list.append(doc_style)\n",
    "\n",
    "            if avail_code != 'S' and doc_style != 'pdf':\n",
    "                url = url_item.xpath('url/text()')[0]\n",
    "                other_links_list.append(url)\n",
    "\n",
    "        if 'pdf' in doc_style_list:\n",
    "            for url_item in ft_url_list:\n",
    "                doc_style = url_item.xpath('documentStyle/text()')[0]\n",
    "                if doc_style == 'pdf':\n",
    "                    url = url_item.xpath('url/text()')[0]\n",
    "                    pdf_links_list.append(url)                        \n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        avail_codes = ', '.join(set(avail_code_list)) if avail_code_list else None\n",
    "        other_links = ', '.join(other_links_list) if other_links_list else None\n",
    "        pdf_links = ', '.join(pdf_links_list) if pdf_links_list else None\n",
    "        \n",
    "        #cursor.execute(\"insert or ignore into result_ids(query_id, pmid) values(?,?)\", (current_query_id, pmid))\n",
    "        \n",
    "        cursor.execute(\"insert or ignore into article_data(pmid, year, title, abstract, journal_title, journal_abbrev_title, in_epmc, avail_codes, pdf_links, other_links) values (?,?,?,?,?,?,?,?,?,?)\", (pmid, year, title, abstract, journal_title, journal_abbrev_title, in_ePMC, avail_codes, pdf_links, other_links))\n",
    "        \n",
    "        conn.commit()\n",
    "        \n",
    "        return None\n",
    "\n",
    "    tree = do_query(query)\n",
    " \n",
    "    result_hitcount = int(tree.xpath('/responseWrapper/hitCount/text()')[0]) \n",
    "    cursor.execute(\"insert into queries(query_id, query, hitcount, date_performed) values (NULL,?,?,?)\", (query, result_hitcount, current_date))\n",
    "    current_query_id = cursor.lastrowid\n",
    "\n",
    "    result_abstracts = tree.xpath('/responseWrapper/resultList/result')\n",
    "    \n",
    "    cursor.execute('select distinct pmid from article_data')\n",
    "    existing_pmid_list = [i[0] for i in cursor.fetchall()]\n",
    "    #print(len(existing_pmid_list))\n",
    "    #print(existing_pmid_list[:10])\n",
    "\n",
    "    for item in result_abstracts:\n",
    "        try:\n",
    "            pmid = int(item.xpath('pmid/text()')[0])\n",
    "            #print(pmid)\n",
    "            cursor.execute(\"insert or ignore into result_ids(query_id, pmid) values(?,?)\", (current_query_id, pmid))\n",
    "\n",
    "            if (pmid in existing_pmid_list) == True:\n",
    "                #exist_count += 1\n",
    "                continue\n",
    "            else:\n",
    "                try:\n",
    "                    retrieve_data(item)\n",
    "                    #new_inserted_count += 1\n",
    "                except IndexError:\n",
    "                    object_id = item.xpath('id/text()')[0]\n",
    "                    error_comment = '(get_pmids_and_article_data) - IndexError with XML, possibly field not present, e.g. no journal info e.g. when is book chapter'\n",
    "                    cursor.execute(\"insert or ignore into error_records(query_id, object_id, error_comment) values (?,?,?)\", (current_query_id, object_id, error_comment))\n",
    "                    continue\n",
    "        except IndexError:\n",
    "            object_id = item.xpath('id/text()')[0]\n",
    "            error_comment = '(get_pmids_and_article_data) - IndexError with XML, possibly no pmid for this item or no journal info e.g. when is book chapter'\n",
    "            cursor.execute(\"insert or ignore into error_records(query_id, object_id, error_comment) values (?,?,?)\", (current_query_id, object_id, error_comment))\n",
    "            continue\n",
    "        \n",
    "    if (result_hitcount % 25 == 0) and (result_hitcount / 25 > 1): # hitCount is multiple of 25 (remainder==0) but more than one page\n",
    "        total_pages = int(result_hitcount / 25)\n",
    "\n",
    "    else:  \n",
    "        total_pages = (int(result_hitcount // 25)) + 1\n",
    "\n",
    "    for page in range(2,total_pages+1): #page one was already done so continue from page 2 and add results to same dictionary\n",
    "\n",
    "        tree = do_query(query, page_nr = page)\n",
    "        result_abstracts = tree.xpath('/responseWrapper/resultList/result')\n",
    "\n",
    "        for item in result_abstracts:\n",
    "            try:\n",
    "                pmid = int(item.xpath('pmid/text()')[0])\n",
    "                cursor.execute(\"insert or ignore into result_ids(query_id, pmid) values(?,?)\", (current_query_id, pmid))\n",
    "\n",
    "                if pmid in existing_pmid_list:\n",
    "                    #exist_count += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    try:\n",
    "                        retrieve_data(item)\n",
    "                        #new_inserted_count += 1\n",
    "                    except IndexError:\n",
    "                        object_id = item.xpath('id/text()')[0]\n",
    "                        error_comment = '(get_pmids_and_article_data) - IndexError with XML, possibly field not present, e.g. no journal info e.g. when is book chapter'\n",
    "                        cursor.execute(\"insert or ignore into error_records(query_id, object_id, error_comment) values (?,?,?)\", (current_query_id, object_id, error_comment))\n",
    "                        continue\n",
    "            except IndexError:\n",
    "                object_id = item.xpath('id/text()')[0]\n",
    "                error_comment = '(get_pmids_and_article_data) - IndexError with XML, possibly no pmid for this item or no journal info e.g. when is book chapter'\n",
    "                cursor.execute(\"insert or ignore into error_records(query_id, object_id, error_comment) values (?,?,?)\", (current_query_id, object_id, error_comment))\n",
    "                continue\n",
    "            \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    #print(new_inserted_count, exist_count)\n",
    "    \n",
    "    return current_query_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_scores(query_id, db_name):\n",
    "    '''For given query_id rank corresponding titles and abstracts using ChEMBL HeCaToS webservice (ChEMBL-likeness score). \n",
    "    Save in scores table. Excludes any pmids for which there is already a score in the scores table.\n",
    "    If no score could be retrieved, is saved in error table together with pmid and query_id\n",
    "    kwargs: query_id -- query_id from queries table, db_name'''\n",
    "    \n",
    "    title_abstract_dict = {}\n",
    "    URL = 'http://scitegic.windows.ebi.ac.uk:9955/rest/HeCaToS_ChEMBLLIKE/{}/{}'\n",
    "    score_dict = {}\n",
    "    #my_count = 0\n",
    "\n",
    "    conn = lite.connect(db_name)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    cursor.execute('''select pmid, title, abstract from article_data \n",
    "                    where title is not null \n",
    "                    and abstract is not null \n",
    "                    and pmid not in (select distinct pmid from scores) \n",
    "                    and pmid in (select pmid from result_ids where query_id = ?) ''', (query_id,))\n",
    "    #this sql query excludes things that already have a score\n",
    "    \n",
    "    results = cursor.fetchall()\n",
    "    \n",
    "    for item in results:\n",
    "        pmid = item[0]\n",
    "        title = item[1]\n",
    "        abstract = item[2]\n",
    "        title_abstract_dict[pmid]= [title, abstract]\n",
    "    \n",
    "    for article_pmid in title_abstract_dict:\n",
    "        \n",
    "        try:\n",
    "            title = title_abstract_dict[article_pmid][0]\n",
    "            abstract = title_abstract_dict[article_pmid][1]\n",
    "    \n",
    "            pre_title = str(base64.b64encode(title.encode('utf-8')))\n",
    "            post_title = re.search(\"^b'(.*)'$\", pre_title).group(1)\n",
    "    \n",
    "            pre_abstract = str(base64.b64encode(abstract.encode('utf-8')))\n",
    "            post_abstract = re.search(\"^b'(.*)'$\", pre_abstract).group(1)\n",
    "    \n",
    "            response_0 = requests.get(URL.format(post_title, post_abstract))\n",
    "            response = response_0.json()  \n",
    "            \n",
    "            cl_score = float(response['score'])\n",
    "            cursor.execute(\"insert or ignore into scores(pmid, score) values(?,?)\", (article_pmid, cl_score))\n",
    "            #my_count += 1\n",
    "            \n",
    "        except ValueError:\n",
    "            error_info = '(get scores) error, status code: '+str(response_0.status_code)\n",
    "            cursor.execute(\"insert or ignore into error_records(query_id, pmid, error_comment) values (?,?,?)\", (query_id, article_pmid, error_info))\n",
    "        except AttributeError:\n",
    "            error_info = '(get scores) AttributeError'\n",
    "            cursor.execute(\"insert or ignore into error_records(query_id, pmid, error_comment) values (?,?,?)\", (query_id, article_pmid, error_info))\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    #print(my_count)\n",
    "    return None\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_chembl_values(query_id, db_name):\n",
    "    '''This function updates the article_data table and sets the in_chembl field by comparing pmid with chembl_pmids table. Needs to be done for each query_id for the sorting in the get_df to work well.\n",
    "    kwargs:\n",
    "            query_id\n",
    "            db_name'''\n",
    "    \n",
    "    conn = lite.connect(db_name)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    cursor.execute('update article_data set in_chembl = 1 where pmid in (select pmid from result_ids where query_id = ?) and pmid in (select distinct pmid from chembl_pmids)', (query_id,))\n",
    "    cursor.execute('update article_data set in_chembl = 0 where pmid in (select pmid from result_ids where query_id = ?) and pmid not in (select distinct pmid from chembl_pmids)', (query_id,))\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_availabilities(query_id, db_name):\n",
    "    '''Want to check access to paper via campus subscriptions for those without 'F' or 'OA' in the availability_codes OR those without any availability codes, of a given query. \n",
    "    Check whether the SFX resolver has link to Full Text available, if so, save link in campus_links field of the article_links table, else, save link to 'request document' form in request_access field in article_links table.\n",
    "    Excludes articles already in ChEMBL. Also skips pubmed ids which have article data already retrieved.\n",
    "    kwargs: query_id -- query_id from queries table in queries_db\n",
    "            db_name -- name of SQLite database'''\n",
    "    \n",
    "    conn = lite.connect(db_name)\n",
    "    cursor = conn.cursor()\n",
    "    avail_dict = {}\n",
    "            \n",
    "    def f(form):\n",
    "    \n",
    "        params = {x.attrib['name']: x.attrib['value'] for x in form.xpath('.//input[@type=\"hidden\"]')}\n",
    "    \n",
    "        response = requests.get('http://wtgcsfx.hosted.exlibrisgroup.com/wtsc/cgi/core/sfxresolver.cgi', params=params)\n",
    "    \n",
    "        return response.status_code, response.url    \n",
    "    \n",
    "    \n",
    "    sql =  '''select pmid from article_data\n",
    "            where ((avail_codes not like '%F%' and avail_codes not like '%OA%') or avail_codes is null)\n",
    "            and pmid in (select pmid from result_ids where query_id = ?)\n",
    "            and pmid not in (select distinct pmid from article_links)\n",
    "            and in_chembl != 1'''\n",
    "    \n",
    "    cursor.execute(sql, (query_id,))\n",
    "   \n",
    "    for pmid in [i[0] for i in cursor.fetchall()]:\n",
    "        \n",
    "        try:\n",
    "        \n",
    "            for attempt_number in range(3):\n",
    "\n",
    "                response = requests.get('http://wtgcsfx.hosted.exlibrisgroup.com/wtsc?sid=Entrez:PubMed&id=pmid:{}'.format(pmid))\n",
    "\n",
    "                if response.status_code == 200:\n",
    "                    tree = lxml.html.fromstring(response.text)\n",
    "                    ft_avail = tree.xpath('//div[@class=\"service\"]/text()')\n",
    "                    break\n",
    "\n",
    "                else:\n",
    "                    sleep(2)\n",
    "                    continue\n",
    "\n",
    "\n",
    "            if response.status_code == 200:\n",
    "\n",
    "                if 'No Full text available' in str(ft_avail):\n",
    "\n",
    "                    try:\n",
    "                        #raise requests.TooManyRedirects('oh no')\n",
    "                        responses = [f(x) for x in tree.xpath('//table[@id=\"service_type_header_getDocumentDelivery\"]//form[contains(@name, \"basic\")]')]\n",
    "\n",
    "                        urls = [response_tuple[1] for response_tuple in responses if response_tuple[0] == 200]\n",
    "\n",
    "                        if not urls:\n",
    "                            error_info = '(get_availability) Case 1: No URL with status_code = 200 available.'\n",
    "                            cursor.execute(\"insert or ignore into error_records(query_id, pmid, error_comment) values (?,?,?)\", (query_id, pmid, error_info))\n",
    "                            continue\n",
    "\n",
    "                        else:\n",
    "                            urls_string = ', '.join(urls)\n",
    "                            cursor.execute('insert into article_links(pmid, request_access) values(?,?)', (pmid, urls_string))\n",
    "                            conn.commit()\n",
    "                            continue\n",
    "\n",
    "                    except requests.TooManyRedirects:\n",
    "                        print(pmid, ' -- TooManyRedirects')\n",
    "                        error_info = '(get_availability) TooManyRedirects'\n",
    "                        cursor.execute(\"insert or ignore into error_records(query_id, pmid, error_comment) values (?,?,?)\", (query_id, pmid, error_info))\n",
    "                        conn.commit()\n",
    "                        continue\n",
    "\n",
    "                if not ft_avail:\n",
    "                    error_info = '(get_availability) Case 2: ft_avail is empty. Not the usual SFX page, possibly SFX Multiple Object Menu, suggest to go to the page manually'\n",
    "                    cursor.execute(\"insert or ignore into error_records(query_id, pmid, error_comment) values(?,?,?)\", (query_id, pmid, error_info))\n",
    "                    continue\n",
    "\n",
    "                if ('No Full text available' not in str(ft_avail) and 'Request document via' in str(ft_avail)):\n",
    "\n",
    "                    try:\n",
    "                        #raise requests.TooManyRedirects('oh no')\n",
    "                        responses = [f(x) for x in tree.xpath('//table[@id=\"service_type_header_getFullTxt\"]//form[contains(@name, \"basic\")]')]\n",
    "\n",
    "                        urls = [response_tuple[1] for response_tuple in responses if response_tuple[0] == 200]\n",
    "\n",
    "                        if not urls:\n",
    "\n",
    "                            try:\n",
    "                                responses = [f(x) for x in tree.xpath('//table[@id=\"service_type_header_getDocumentDelivery\"]//form[contains(@name, \"basic\")]')]\n",
    "                                urls = [response_tuple[1] for response_tuple in responses if response_tuple[0] == 200]\n",
    "\n",
    "                                if not urls:\n",
    "                                    error_info = '(get_availability) Case 1b: No URL with status_code = 200 available.'\n",
    "                                    cursor.execute(\"insert or ignore into error_records(query_id, pmid, error_comment) values (?,?,?)\", (query_id, pmid, error_info))\n",
    "                                    continue\n",
    "\n",
    "                                else:\n",
    "                                    urls_string = ', '.join(urls)\n",
    "                                    cursor.execute('insert into article_links(request_access, pmid) values(?,?)', (urls_string, pmid))\n",
    "                                    conn.commit()\n",
    "                                    continue\n",
    "\n",
    "                            except requests.TooManyRedirects:\n",
    "                                #print(pmid, ' -- TooManyRedirects')\n",
    "                                error_info = '(get_availability) TooManyRedirects'\n",
    "                                cursor.execute(\"insert or ignore into error_records(query_id, pmid, error_comment) values (?,?,?)\", (query_id, pmid, error_info))\n",
    "                                conn.commit()\n",
    "                                continue\n",
    "\n",
    "                            #error_info = '(get_availability) Case 3: No campus ft-URL with status_code = 200 available.'\n",
    "                            #cursor.execute(\"insert or ignore into other_error(query_id, pmid, error_comment) values (?,?,?)\", (query_id, pmid, error_info))\n",
    "                            #continue\n",
    "\n",
    "                        else:\n",
    "                            urls_string = ', '.join(urls)\n",
    "                            cursor.execute('insert into article_links(campus_links, pmid) values (?,?)', (urls_string, pmid))\n",
    "                            conn.commit()\n",
    "                            continue\n",
    "\n",
    "                    except requests.TooManyRedirects:\n",
    "                        error_info = '(get_availability) TooManyRedirects'\n",
    "                        cursor.execute(\"insert or ignore into error_records(query_id, pmid, error_comment) values (?,?,?)\", (query_id, pmid, error_info))\n",
    "                        conn.commit()\n",
    "                        continues\n",
    "\n",
    "\n",
    "                else:\n",
    "                    error_info = \"(get_availability) Case 4: Could not resolve access for article strings that were tested were not present in response.\"\n",
    "                    cursor.execute('insert or ignore into error_records(query_id, pmid, error_comment) values (?,?,?)', (query_id, pmid, error_info))\n",
    "                    continue\n",
    "\n",
    "            else:\n",
    "                error_info = \"(get_availability) Case 5: could not get status code 200 from sfx page for this pmid after 3 attempts.\"\n",
    "                cursor.execute('insert or ignore into error_records(query_id, pmid, error_comment) values (?,?,?)', (query_id, pmid, error_info))\n",
    "                continue\n",
    "        \n",
    "        except (TypeError, requests.ConnectionError):\n",
    "            print('oops, not going well there', pmid)\n",
    "            continue\n",
    "        except:\n",
    "            print(\"Unexpected error:\", sys.exc_info()[0], pmid)\n",
    "            continue\n",
    "            \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "            \n",
    "    return None\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_df(query_id_list, db_name, sql_condition = None):\n",
    "    '''Makes a dataframe for a given query/queries. Can include multiple query_ids from queries table. If only one is needed put that one item in a list. \n",
    "    Selects following information on results for a given query from the queries_db: pmid, year, title, abstract, in_chembl, score, availability_codes, pdf_links, campus_links, request_access.\n",
    "    Then sort the dataframe. First sorts on in_chembl, availability (subscription without access last), then on scores. Campus_links are only displayed if full text available from subscription.\n",
    "    At the end of the dataframe the abstracts that had an error are displayed with an error_comment. The error_comment lists the function when the error occurred.\n",
    "    Returns a tuple with first a non-HTML data frame and second an HTML version of the dataframe. Save the non-HTML version for use in the colour_terms function.\n",
    "    It is possible to add further condition(s) to the sql statement by using the sql_condition argument. One \"and\" will be inserted by the function, then can add e.g. 's.score > 10', \n",
    "    which will then be appended to the sql statement.\n",
    "    kwargs: query_id_list - list of query_ids to be included.\n",
    "            db_name -- name of SQLite database\n",
    "            sql_condition -- a further condition to be appended to the sql statement. One 'and' will be included by the function, so write the condition straight away. Default = None'''\n",
    "        \n",
    "    conn = lite.connect(db_name)\n",
    "    cursor = conn.cursor()\n",
    "    pd.set_option('max_colwidth',100000)\n",
    "    \n",
    "    sql = '''select distinct r.pmid, a.year, a.title, a.abstract, a.journal_title, a.in_chembl, s.score, a.avail_codes, a.pdf_links, a.other_links, al.campus_links, al.request_access, er.error_comment\n",
    "        from result_ids r\n",
    "        left join article_data a on r.pmid = a.pmid\n",
    "        left join scores s on a.pmid = s.pmid\n",
    "        left join article_links al on a.pmid = al.pmid\n",
    "        left join error_records er on (r.query_id = er.query_id and r.pmid = er.pmid)\n",
    "        where r.query_id in ({})\n",
    "        '''\n",
    "    \n",
    "    if sql_condition != None:\n",
    "        sql = sql+' and '+sql_condition\n",
    "    \n",
    "    \n",
    "    def make_into_links(x, link_type):\n",
    "        if x:\n",
    "            link_list = x.split(', ')\n",
    "            new_link_list = []\n",
    "            if link_list:\n",
    "                for index,i in enumerate(link_list):\n",
    "                    item = '<a target=\"_blank\" href=\"{}\">{}_link_{}</a>'.format(i, link_type, index)\n",
    "                    new_link_list.append(item)\n",
    "            return ', '.join(new_link_list)\n",
    "    \n",
    "    sql_result_list = []\n",
    "    \n",
    "    query_id = str(query_id_list).strip('[]')\n",
    "        \n",
    "    for row in cursor.execute(sql.format(query_id)):\n",
    "        sql_result_list.append(row)\n",
    "    \n",
    "                              \n",
    "    try:\n",
    "        df = pd.DataFrame(sql_result_list, columns = ['pmid', 'year', 'title', 'abstract', 'journal', 'inChEMBL', 'score', 'avail', 'pdf_links', 'other_links', 'campus_links', 'request_access', 'error_comment'])\n",
    "\n",
    "        df['ePMC_link'] = df['pmid'].apply(lambda x: '<a target=\"_blank\" href=\"http://europepmc.org/abstract/MED/{}\">ePMC link</a>'.format(x))\n",
    "        \n",
    "        df['pdf_links'] = df['pdf_links'].apply(make_into_links, link_type = 'pdf')\n",
    "        df['pdf_links_boolean'] = df['pdf_links'].apply(lambda x: 1 if x else 0)\n",
    "        df['pdf_links'] = df['pdf_links'].apply(lambda x: x if x else '')\n",
    "        \n",
    "        df['other_links'] = df['other_links'].apply(make_into_links, link_type = 'other')\n",
    "        df['other_links_boolean'] = df['other_links'].apply(lambda x: 1 if x else 0)\n",
    "        df['other_links'] = df['other_links'].apply(lambda x: x if x else '')\n",
    "        \n",
    "        df['campus_links'] = df['campus_links'].apply(make_into_links, link_type = 'campus')\n",
    "        df['campus_links_boolean'] = df['campus_links'].apply(lambda x: 1 if x else 0)\n",
    "        df['campus_links'] = df['campus_links'].apply(lambda x: x if x else '')\n",
    "        \n",
    "        df['request_access'] = df['request_access'].apply(make_into_links, link_type = 'request_access')\n",
    "        df['request_access_boolean'] = df['request_access'].apply(lambda x: 1 if x else 0)\n",
    "        df['request_access'] = df['request_access'].apply(lambda x: x if x else '')\n",
    "        \n",
    "        df['avail'] = df['avail'].apply(lambda x: x if x else '')\n",
    "        \n",
    "        df['error_comment'] = df['error_comment'].apply(lambda x: x if x else '')\n",
    "        \n",
    "        df['pmid_link'] = df['pmid'].apply(lambda x: '<a target=\"_blank\" href=\"http://www.ncbi.nlm.nih.gov/pubmed/?term={}\">{}</a>'.format(x,x))\n",
    "                                              \n",
    "        #df = df.convert_objects(convert_numeric=True)\n",
    "\n",
    "        df.sort(columns = ['inChEMBL', 'error_comment', 'request_access_boolean', 'score', 'pdf_links_boolean', 'other_links_boolean', 'campus_links_boolean'], ascending = [True, True, True, False, False, False, False], inplace = True)\n",
    "        df.index = range(1,len(df) + 1)\n",
    "        \n",
    "        df = df.loc[:,['pmid', 'pmid_link', 'year', 'title', 'abstract', 'journal', 'inChEMBL', 'score', 'avail', 'pdf_links', 'other_links', 'campus_links', 'request_access', 'error_comment']]\n",
    "\n",
    "        conn.close()\n",
    "    \n",
    "        return df, HTML(df.to_html(escape=False))\n",
    "\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        print('probably no results for this query, empty db table or problem with df e.g. sorting')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def colour_terms(df, markup_list):\n",
    "    '''Will colour the terms in the df according to the markup_list and return an HTML object with the colours. Supply a df(non-HTML) and list of dictionaries. Each dictionary should have keys 'name'(of dict), 'terms'(value is list of terms), and 'colour'.\n",
    "    kwargs: df -- a dataframe with a column 'abstracts' and 'title'\n",
    "            markup_list -- a list of dictionaries with markup specification (list of terms and colour assigned)\n",
    "    '''\n",
    "    df = df.copy()\n",
    "    \n",
    "    css = '\\n'.join(\".{name} {{ color: {color}; }}\".format(**x) for x in markup_list)\n",
    "\n",
    "    def add_colour(html):\n",
    "        \n",
    "        for x in markup_list:\n",
    "\n",
    "            # pattern = re.compile(r'\\b(' + r'|'.join(x['terms']) + r')\\b', flags=re.I)\n",
    "            \n",
    "            pattern = re.compile('(' + '|'.join(x['terms']) + ')', flags=re.I)\n",
    "\n",
    "            html = re.sub(pattern, r'<span class=\"{}\">\\1</span>'.format(x['name']), html)\n",
    "            \n",
    "        return html\n",
    "\n",
    "    df['abstract'] = df['abstract'].apply(add_colour)\n",
    "    df['title'] = df['title'].apply(add_colour)\n",
    "    \n",
    "    return HTML('<style>' + css + '</style>' + df.to_html(escape=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def separate_column_df(query_id_list, db_name, sql_condition = None):\n",
    "    '''Same as get_df function only now pdf_links and other_links have been split up in separate column for each link.\n",
    "    Get_df docstring: Selects following information on results for a given query from the queries_db: pmid, year, title, abstract, in_chembl, score, availability_codes, pdf_links, campus_links, request_access.\n",
    "    Then sort the dataframe. First sorts on in_chembl, availability (subscription without access last), then on scores. Campus_links are only displayed if full text available from subscription.\n",
    "    At the end of the dataframe the abstracts that had an error are displayed with an error_comment. The error_comment lists the function when the error occurred.\n",
    "    It is possible to add further condition(s) to the sql statement by using the sql_condition argument. One \"and\" will be inserted by the function, then can add e.g. 's.score > 10', \n",
    "    which will then be appended to the sql statement.\n",
    "    Returns a tuple with first a non_HTML data frame and second an HTML version of the dataframe.\n",
    "    kwargs: query_id_list -- query_id from queries table in queries_db\n",
    "            db_name -- Name of SQLite database\n",
    "            sql_condition -- a further condition to be appended to the sql statement. One 'and' will be included by the function, so write the condition straight away. Default = None'''\n",
    "    \n",
    "    conn = lite.connect(db_name)\n",
    "    cursor = conn.cursor()\n",
    "    pd.set_option('max_colwidth',100000)\n",
    "    \n",
    "    sql = '''select distinct r.pmid, a.year, a.title, a.abstract, a.journal_title, a.in_chembl, s.score, a.avail_codes, a.pdf_links, a.other_links, al.campus_links, al.request_access, er.error_comment\n",
    "        from result_ids r\n",
    "        left join article_data a on r.pmid = a.pmid\n",
    "        left join scores s on a.pmid = s.pmid\n",
    "        left join article_links al on a.pmid = al.pmid\n",
    "        left join error_records er on (r.query_id = er.query_id and r.pmid = er.pmid)\n",
    "        where r.query_id in ({})\n",
    "        '''\n",
    "    \n",
    "    if sql_condition != None:\n",
    "        sql = sql+' and '+sql_condition\n",
    "    \n",
    "    def make_into_links(x, link_type):\n",
    "        if x:\n",
    "            link_list = x.split(', ')\n",
    "            new_link_list = []\n",
    "            if link_list:\n",
    "                for index,i in enumerate(link_list):\n",
    "                    item = '<a target=\"_blank\" href=\"{}\">{}_link_{}</a>'.format(i, link_type, index)\n",
    "                    new_link_list.append(item)\n",
    "            return ', '.join(new_link_list)\n",
    "    \n",
    "    sql_result_list = []\n",
    "    \n",
    "    query_id = str(query_id_list).strip('[]')\n",
    "    \n",
    "    for row in cursor.execute(sql.format(query_id)):\n",
    "        sql_result_list.append(row)\n",
    "    \n",
    "    try:\n",
    "        df = pd.DataFrame(sql_result_list, columns = ['pmid', 'year', 'title', 'abstract', 'journal', 'inChEMBL', 'score', 'avail', 'pdf_links', 'other_links', 'campus_links', 'request_access', 'error_comment'])\n",
    "\n",
    "        df['ePMC_link'] = df['pmid'].apply(lambda x: '<a target=\"_blank\" href=\"http://europepmc.org/abstract/MED/{}\">ePMC link</a>'.format(x))\n",
    "        \n",
    "        df['pdf_links'] = df['pdf_links'].apply(make_into_links, link_type = 'pdf')\n",
    "        df['pdf_links_boolean'] = df['pdf_links'].apply(lambda x: 1 if x else 0)\n",
    "        df['pdf_links'] = df['pdf_links'].apply(lambda x: x if x else '')\n",
    "        \n",
    "        df['other_links'] = df['other_links'].apply(make_into_links, link_type = 'other')\n",
    "        df['other_links_boolean'] = df['other_links'].apply(lambda x: 1 if x else 0)\n",
    "        df['other_links'] = df['other_links'].apply(lambda x: x if x else '')\n",
    "        \n",
    "        df['campus_links'] = df['campus_links'].apply(make_into_links, link_type = 'campus')\n",
    "        df['campus_links_boolean'] = df['campus_links'].apply(lambda x: 1 if x else 0)\n",
    "        df['campus_links'] = df['campus_links'].apply(lambda x: x if x else '')\n",
    "        \n",
    "        df['request_access'] = df['request_access'].apply(make_into_links, link_type = 'request_access')\n",
    "        df['request_access_boolean'] = df['request_access'].apply(lambda x: 1 if x else 0)\n",
    "        df['request_access'] = df['request_access'].apply(lambda x: x if x else '')\n",
    "        \n",
    "        df['avail'] = df['avail'].apply(lambda x: x if x else '')\n",
    "        \n",
    "        df['error_comment'] = df['error_comment'].apply(lambda x: x if x else '')\n",
    "        \n",
    "        df['pmid'] = df['pmid'].apply(lambda x: '<a target=\"_blank\" href=\"http://www.ncbi.nlm.nih.gov/pubmed/?term={}\">{}</a>'.format(x,x))\n",
    "                                              \n",
    "        #df = df.convert_objects(convert_numeric=True)\n",
    "\n",
    "        df.sort(columns = ['inChEMBL', 'error_comment', 'request_access_boolean', 'score', 'pdf_links_boolean', 'other_links_boolean', 'campus_links_boolean'], ascending = [True, True, True, False, False, False, False], inplace = True)\n",
    "        df.index = range(1,len(df) + 1)\n",
    "        \n",
    "        # get highest number of pdf links in the table\n",
    "        pdf_links_lengths_list = []\n",
    "        df['pdf_links'].apply(lambda x: pdf_links_lengths_list.append(len(x.split(', '))))\n",
    "\n",
    "        # insert each pdf link into separate column\n",
    "        for number in range(0,max(pdf_links_lengths_list)):\n",
    "            column_name = 'pdf-'+str(number)\n",
    "            df = pd.concat([df, df['pdf_links'].apply(lambda x: pd.Series({column_name:x.split(', ')[number] if (len(x.split(', ')) > number) else ''}))], axis = 1)\n",
    "    \n",
    "        # get names of columns that were added to the table\n",
    "        pdf_link_columns = []\n",
    "        for i in df.columns:\n",
    "            if 'pdf-' in i:\n",
    "                pdf_link_columns.append(i)\n",
    "            \n",
    "        # repeat process done on pdf_links on other_links\n",
    "        \n",
    "        \n",
    "        # get highest number of pdf links in the table\n",
    "        other_links_lengths_list = []\n",
    "        df['other_links'].apply(lambda x: other_links_lengths_list.append(len(x.split(', '))))\n",
    "\n",
    "        # insert each other link into separate column\n",
    "        for number in range(0,max(other_links_lengths_list)):\n",
    "            column_name = 'other-'+str(number)\n",
    "            df = pd.concat([df, df['other_links'].apply(lambda x: pd.Series({column_name:x.split(', ')[number] if (len(x.split(', ')) > number) else ''}))], axis = 1)\n",
    "    \n",
    "        # get names of columns that were added to the table\n",
    "        other_link_columns = []\n",
    "        for i in df.columns:\n",
    "            if 'other-' in i:\n",
    "                other_link_columns.append(i)\n",
    "        \n",
    "        \n",
    "        #subset dataframe to get relevant columns only\n",
    "        \n",
    "        column_list = ['pmid', 'year', 'title', 'abstract', 'journal', 'inChEMBL', 'score', 'avail', 'pdf_links_boolean']\n",
    "        \n",
    "        for item in pdf_link_columns:\n",
    "            column_list.append(item)\n",
    "        \n",
    "        for item in other_link_columns:\n",
    "            column_list.append(item)\n",
    "            \n",
    "        for item in \"campus_links, request_access, error_comment\".split(', '):\n",
    "            column_list.append(item)\n",
    "        \n",
    "        print(column_list)\n",
    "        df = df.loc[:,column_list]\n",
    "\n",
    "        conn.close()\n",
    "    \n",
    "        return df, HTML(df.to_html(escape=False))\n",
    "\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        print('probably no results for this query, empty db table or problem with df e.g. sorting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_scores(query_id_list, db_name, figure_title):\n",
    "    '''Creates a histogram of the chembl-likeness-scores for a given query/queries. Will plot inline.\n",
    "    kwargs: query_id_list\n",
    "            db_name\n",
    "            figure_title'''\n",
    "    \n",
    "    %matplotlib inline\n",
    "    \n",
    "    conn = lite.connect(db_name)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    query_id = str(query_id_list).strip('[]')\n",
    "    cursor.execute('select score from scores where pmid in (select distinct pmid from result_ids where query_id in ({}))'.format(query_id))\n",
    "    \n",
    "    score_list = [i[0] for i in cursor.fetchall()]\n",
    "    score_array = np.array(score_list)\n",
    "    \n",
    "    plt.hist(score_array)\n",
    "    plt.xlabel('ChEMBL-likeness score')\n",
    "    plt.ylabel('Number of abstracts')\n",
    "    plt.title('Query: {}\\n'.format(figure_title)+'Total number of distinct abstracts ranked: '+str(len(score_list)))\n",
    "    \n",
    "    conn.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
